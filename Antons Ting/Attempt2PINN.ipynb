{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "# from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from Load_data2 import custom_csv_parser\n",
    "torch.set_default_dtype(torch.float)\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda':\n",
    "    print(torch.cuda.get_device_name())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss ODE: 32.922703, Loss data: 27352.183594\n",
      "Epoch 1000, Loss ODE: 26.644047, Loss data: 3.729960\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 215\u001b[0m\n\u001b[0;32m    213\u001b[0m loss_ode, loss_data \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss(t_train, t_train_data, u_train, d_train, data_train, scalar)\n\u001b[0;32m    214\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_ode \u001b[38;5;241m+\u001b[39m loss_data\n\u001b[1;32m--> 215\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    217\u001b[0m \u001b[38;5;66;03m# scheduler.step()\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# Get current learning rate\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anton\\OneDrive\\Noter\\Deep Learning\\Projekt\\DeepLearning-PINNs\\DeepLearning-PINNs\\DeepLearningENV\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anton\\OneDrive\\Noter\\Deep Learning\\Projekt\\DeepLearning-PINNs\\DeepLearning-PINNs\\DeepLearningENV\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anton\\OneDrive\\Noter\\Deep Learning\\Projekt\\DeepLearning-PINNs\\DeepLearning-PINNs\\DeepLearningENV\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def grad(func, var):\n",
    "    '''\n",
    "    Computes the gradient of a function with respect to a variable.\n",
    "    Written by Engsig-Karup, Allan P. (07/05/2024).\n",
    "\n",
    "    Args:\n",
    "    func (torch.Tensor): Function to differentiate\n",
    "    var (torch.Tensor): Variable to differentiate with respect to\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Gradient of func with respect to var\n",
    "    '''\n",
    "    return torch.autograd.grad(func, var, grad_outputs=torch.ones_like(func), create_graph=True, retain_graph=True)[0] \n",
    "\n",
    "class PINN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=1, hidden_dim=20, output_dim=7, num_hidden=1):\n",
    "        super().__init__()               \n",
    "        self.input = nn.Linear(in_features=input_dim, out_features=hidden_dim)\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "        self.hidden = nn.ModuleList()\n",
    "        for _ in range(num_hidden):\n",
    "            self.hidden.append(nn.Linear(in_features=hidden_dim, out_features=hidden_dim))\n",
    "\n",
    "        self.output = nn.Linear(in_features=hidden_dim, out_features=output_dim)\n",
    "        # Parameters for the ODEs\n",
    "        self.tau_1 = torch.tensor([49.0], requires_grad=True, device=device)       # [min]\n",
    "        self.tau_2 = torch.tensor([47.0], requires_grad=True, device=device)       # [min]\n",
    "        self.C_I = torch.tensor([20.1], requires_grad=True, device=device)         # [dL/min]\n",
    "        self.p_2 = torch.tensor([0.0106], requires_grad=True, device=device)       # [min^(-1)]\n",
    "        self.GEZI = torch.tensor([0.0022], requires_grad=True, device=device)      # [min^(-1)]\n",
    "        self.EGP_0 = torch.tensor([1.33], requires_grad=True, device=device)       # [(mg/dL)/min]\n",
    "        self.V_G = torch.tensor([253.0], requires_grad=True, device=device)        # [dL]\n",
    "        self.tau_m = torch.tensor([47.0], requires_grad=True, device=device)       # [min]\n",
    "        self.tau_sc = torch.tensor([5.0], requires_grad=True, device=device)       # [min]\n",
    "        # self.S_I = torch.tensor([0.0081], requires_grad=True, device=device)\n",
    "\n",
    "    def forward(self, t):\n",
    "        u = self.activation(self.input(t))\n",
    "\n",
    "        for hidden_layer in self.hidden:\n",
    "            u = self.activation(hidden_layer(u))\n",
    "\n",
    "        u = self.output(u)\n",
    "        return u\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Initialize weights using Xavier initialization and biases to zero\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def LossODE(self,t,u,d,scalar):\n",
    "\n",
    "        X = self.forward(t)\n",
    "\n",
    "        D_1 = X[:,0]\n",
    "        D_2 = X[:,1]\n",
    "        I_sc = X[:,2]\n",
    "        I_p = X[:,3]\n",
    "        I_eff = X[:,4]\n",
    "        G = X[:,5]\n",
    "        G_sc = X[:,6]\n",
    "\n",
    "        tau_1 = self.tau_1\n",
    "        tau_2 = self.tau_2\n",
    "        C_I = self.C_I\n",
    "        p_2 = self.p_2\n",
    "        GEZI = self.GEZI\n",
    "        EGP_0 = self.EGP_0\n",
    "        V_G = self.V_G\n",
    "        tau_m = self.tau_m\n",
    "        tau_sc = self.tau_sc\n",
    "        # S_I = self.S_I # The special parameter we want to predict.\n",
    "        ## Define the differential equations\n",
    "\n",
    "        D_1_t = grad(D_1,t)\n",
    "        D_2_t = grad(D_2,t)\n",
    "\n",
    "        I_sc_t = grad(I_sc,t)\n",
    "        I_p_t = grad(I_p,t)\n",
    "        I_eff_t = grad(I_eff,t)\n",
    "\n",
    "        G_t = grad(G,t)\n",
    "        G_sc_t = grad(G_sc,t)\n",
    "\n",
    "        Meal_1 = D_1_t - d + (D_1 / tau_m)\n",
    "        Meal_2 = D_2_t - (D_1 / tau_m) + (D_2 / tau_m)\n",
    "\n",
    "        Insulin1 = I_sc_t - (u/(tau_1*C_I)) + (I_sc / tau_1)\n",
    "        Insulin2 = I_p_t - (I_sc / tau_2) + (I_p / tau_2)\n",
    "        Insulin3 = I_eff_t + p_2 * I_eff - p_2 * S_I * I_p\n",
    "\n",
    "        Glucose1 = G_t + (GEZI + I_eff) * G - EGP_0 - ((1000 * D_2) / (V_G * tau_m))\n",
    "        Glucose2 = G_sc_t - (G / tau_sc) + (G_sc / tau_sc)\n",
    "\n",
    "        loss_ode = (1/scalar[0]*torch.mean((Meal_1)**2)   + \n",
    "                    1/scalar[1]*torch.mean(Meal_2**2)     + \n",
    "                    1/scalar[2]*torch.mean(Insulin1**2)   + \n",
    "                    1/scalar[3]*torch.mean(Insulin2**2)   + \n",
    "                    1/scalar[4]*torch.mean(Insulin3**2)   + \n",
    "                    1/scalar[5]*torch.mean(Glucose1**2 )  + \n",
    "                    1/scalar[6]*torch.mean(Glucose2**2))\n",
    "        return loss_ode\n",
    "\n",
    "    def LossData(self,t,data):\n",
    "\n",
    "        X = self.forward(t)\n",
    "\n",
    "        D_1 = X[:,0]\n",
    "        D_2 = X[:,1]\n",
    "        I_sc = X[:,2]\n",
    "        I_p = X[:,3]\n",
    "        I_eff = X[:,4]\n",
    "        G = X[:,5]\n",
    "        G_sc = X[:,6]\n",
    "\n",
    "        D1_data = data[\"D1\"]\n",
    "        D2_data = data[\"D2\"]\n",
    "        I_sc_data = data[\"I_sc\"]\n",
    "        I_p_data = data[\"I_p\"]\n",
    "        I_eff_data = data[\"I_eff\"]\n",
    "        G_data = data[\"G\"]\n",
    "        G_sc_data = data[\"G_sc\"]\n",
    "\n",
    "        data_loss = (torch.mean((D_1 - D1_data)**2) + \n",
    "                     torch.mean((D_2 - D2_data)**2) + \n",
    "                     torch.mean((I_sc - I_sc_data)**2) + \n",
    "                     torch.mean((I_p - I_p_data)**2) + \n",
    "                     torch.mean((I_eff - I_eff_data)**2) + \n",
    "                     torch.mean((G - G_data)**2) + \n",
    "                     torch.mean((G_sc - G_sc_data)**2))\n",
    "        return data_loss\n",
    "\n",
    "    def loss(self,t_train, t_data, u, d, data, scalar):\n",
    "        loss_ode = self.LossODE(t_train, u, d, scalar)\n",
    "        loss_data = self.LossData(t_data, data)\n",
    "\n",
    "        return loss_ode, loss_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check for CUDA availability\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    hidden_dim = 128\n",
    "    num_hidden_layers = 3\n",
    "\n",
    "    # Load data and pre-process\n",
    "    data = custom_csv_parser('../Patient2.csv')\n",
    "    n_data = len(data[\"G\"])\n",
    "\n",
    "    # Split data into training and validation\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    indices = torch.randperm(n_data)\n",
    "\n",
    "    n_train = int(n_data * 0.1)\n",
    "\n",
    "    train_indices = indices[:n_train]\n",
    "    val_indices = indices[n_train:]\n",
    "\n",
    "    # Split the data dictionary \n",
    "    data_train = {}\n",
    "    data_val = {}\n",
    "\n",
    "    for key in data.keys():\n",
    "        data_tensor = torch.tensor(data[key], requires_grad=True, device=device)          # Ensure data is a tensor\n",
    "        data_train[key] = data_tensor[train_indices]\n",
    "        data_val[key] = data_tensor[val_indices]\n",
    "\n",
    "    # Define the model\n",
    "    model = PINN(hidden_dim=hidden_dim, num_hidden=num_hidden_layers).to(device)\n",
    "\n",
    "    S_I = torch.tensor([0.0], requires_grad=True, device=device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()) + [S_I], lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "    num_epoch = 30000\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "      # Collocation points\n",
    "    ### Options for improvement, try and extrend the collocation points after a large sum of training epochs, T + 5 or something\n",
    "    \n",
    "    d_train = data_train[\"Meal\"]\n",
    "    d_val = data_val[\"Meal\"]\n",
    "    \n",
    "    u_train = data_train[\"Insulin\"]\n",
    "    u_val = data_val[\"Insulin\"]\n",
    "    \n",
    "    t_train_data = data_train[\"t\"].reshape(-1, 1)\n",
    "    t_val_data = data_val[\"t\"].reshape(-1, 1)\n",
    "    \n",
    "    T = data[\"t\"][-1]\n",
    "    t_train = torch.linspace(0, T, n_train, requires_grad=True, device=device).reshape(-1, 1)\n",
    "    \n",
    "    \n",
    "    # Setup arrays for saving the losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    learning_rates = []\n",
    "\n",
    "    scalar = torch.Tensor([data_train[\"D1\"].mean(), data_train[\"D2\"].mean(), data_train[\"I_sc\"].mean(), data_train[\"I_p\"].mean(), data_train[\"I_eff\"].mean(), data_train[\"G\"].mean(), data_train[\"G_sc\"].mean()])\n",
    "\n",
    "    # Begin training our model\n",
    "    for epoch in range(num_epoch):\n",
    "        optimizer.zero_grad()\n",
    "        loss_ode, loss_data = model.loss(t_train, t_train_data, u_train, d_train, data_train, scalar)\n",
    "        loss = loss_ode + loss_data\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "        \n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # Create the console output and plot\n",
    "        if epoch % 100 == 0:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                val_loss = model.LossData(t_val_data, data_val)\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            val_losses.append(val_loss.item())\n",
    "            learning_rates.append(current_lr)\n",
    "\n",
    "            # Print training and validation loss\n",
    "        if epoch % 1000 == 0:\n",
    "            # print(f\"Epoch {epoch}, Loss: {loss.item():.6f}, Val Loss: {val_loss.item():.6f}\")\n",
    "            # print(f\"Epoch {epoch}, Loss: {loss.item():.6f}, Val Loss: {val_loss.item():.6f}, S_I: {S_I.item():.6f}\")\n",
    "            # print(f\"Epoch {epoch}, Loss: {loss.item():.6f}, Val Loss: {val_loss.item():.6f}, GEZI: {GEZI.item():.6f}, S_I: {S_I.item():.6f}\")\n",
    "            print(f\"Epoch {epoch}, Loss ODE: {loss_ode.item():.6f}, Loss data: {loss_data.item():.6f}\")#, S_I: {S_I.item():.6f}\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Plot training and validation losses and learning rate\n",
    "    plt.figure(figsize=(18, 5))\n",
    "\n",
    "    # First subplot for losses\n",
    "    plt.subplot(1, 3, 1)\n",
    "    epochs = range(0, num_epoch, 100)  # Since we record losses every 100 epochs\n",
    "    plt.plot(epochs, train_losses, label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.legend()\n",
    "\n",
    "    # Second subplot for learning rate\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(epochs, learning_rates, label='Learning Rate', color='green')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate Schedule')\n",
    "    plt.legend()\n",
    "\n",
    "    # Third subplot for glucose predictions\n",
    "    plt.subplot(1, 3, 3)\n",
    "    t_test = torch.linspace(0, T + 150, 2500, device=device).reshape(-1, 1)\n",
    "    X_pred = model(t_test)\n",
    "    G_pred = X_pred[:, 5].detach().cpu().numpy()\n",
    "\n",
    "    plt.plot(t_test.cpu().numpy(), G_pred, label='Predicted Glucose (G)')\n",
    "    plt.plot(data[\"t\"], data[\"G\"], label='True Glucose (G)')\n",
    "    plt.xlabel('Time (t)')\n",
    "    plt.ylabel('Glucose Level')\n",
    "    plt.title('Predicted vs True Glucose Levels')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearningENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
