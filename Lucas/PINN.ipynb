{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.io\n",
    "\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded successfully!\n",
      "dict_keys(['Meal_size', 'Steady_insulin', 'Bolus', 'D1', 'D2', 'I_sc', 'I_p', 'I_eff', 'G', 'G_sc', 'tau1', 'tau2', 'Ci', 'p2', 'Si', 'GEZI', 'EGP0', 'Vg', 'taum', 'tausc'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[50.0,\n",
       " 44.6809,\n",
       " 39.9276,\n",
       " 35.68,\n",
       " 31.8842,\n",
       " 28.4923,\n",
       " 25.4612,\n",
       " 22.7525,\n",
       " 20.3321,\n",
       " 18.1691,\n",
       " 16.2362,\n",
       " 14.5089,\n",
       " 12.9654,\n",
       " 11.5861,\n",
       " 10.3536,\n",
       " 9.25212,\n",
       " 8.26786,\n",
       " 7.3883,\n",
       " 6.60231,\n",
       " 5.89993,\n",
       " 5.27228,\n",
       " 4.7114,\n",
       " 4.21019,\n",
       " 3.7623,\n",
       " 3.36205,\n",
       " 3.00439,\n",
       " 2.68477,\n",
       " 2.39916,\n",
       " 2.14393,\n",
       " 1.91585,\n",
       " 1.71204,\n",
       " 1.5299,\n",
       " 1.36715,\n",
       " 1.22171,\n",
       " 1.09174,\n",
       " 0.975596,\n",
       " 0.871809,\n",
       " 0.779064,\n",
       " 0.696185,\n",
       " 0.622122,\n",
       " 0.555939,\n",
       " 0.496797,\n",
       " 0.443946,\n",
       " 0.396718,\n",
       " 0.354514,\n",
       " 0.316799,\n",
       " 0.283097,\n",
       " 0.252981,\n",
       " 0.226068,\n",
       " 0.202018,\n",
       " 0.180527,\n",
       " 0.161322,\n",
       " 0.14416,\n",
       " 0.128824,\n",
       " 0.115119,\n",
       " 0.102872,\n",
       " 0.0919285,\n",
       " 0.0821489,\n",
       " 0.0734096,\n",
       " 0.0656001,\n",
       " 0.0586214,\n",
       " 0.0523851,\n",
       " 0.0468122,\n",
       " 0.0418322,\n",
       " 0.0373819,\n",
       " 0.0334051,\n",
       " 0.0298514,\n",
       " 0.0266757,\n",
       " 0.0238379,\n",
       " 0.0213019,\n",
       " 0.0190358,\n",
       " 0.0170107,\n",
       " 0.015201,\n",
       " 0.0135839,\n",
       " 0.0121388,\n",
       " 0.0108474,\n",
       " 0.00969346,\n",
       " 0.00866224,\n",
       " 0.00774073,\n",
       " 0.00691725,\n",
       " 0.00618137,\n",
       " 0.00552378,\n",
       " 0.00493614,\n",
       " 0.00441102,\n",
       " 0.00394176,\n",
       " 0.00352243,\n",
       " 0.0031477,\n",
       " 0.00281284,\n",
       " 0.0025136,\n",
       " 0.0022462,\n",
       " 0.00200724,\n",
       " 0.0017937,\n",
       " 0.00160288,\n",
       " 0.00143236,\n",
       " 0.00127998,\n",
       " 0.00114382,\n",
       " 0.00102213,\n",
       " 0.000913396,\n",
       " 0.000816226,\n",
       " 0.000729393,\n",
       " 0.000651798,\n",
       " 0.000582458,\n",
       " 0.000520494,\n",
       " 0.000465123,\n",
       " 0.000415642,\n",
       " 0.000371424,\n",
       " 0.000331911,\n",
       " 0.000296601,\n",
       " 0.000265048,\n",
       " 0.000236852,\n",
       " 0.000211655,\n",
       " 0.000189138,\n",
       " 0.000169017,\n",
       " 0.000151036,\n",
       " 0.000134969,\n",
       " 0.00012061,\n",
       " 0.00010778,\n",
       " 9.63136e-05,\n",
       " 8.60675e-05,\n",
       " 7.69114e-05,\n",
       " 6.87293e-05,\n",
       " 6.14177e-05,\n",
       " 5.48839e-05,\n",
       " 4.90452e-05,\n",
       " 4.38276e-05,\n",
       " 3.91651e-05,\n",
       " 3.49986e-05,\n",
       " 3.12753e-05,\n",
       " 2.79482e-05,\n",
       " 2.4975e-05,\n",
       " 2.2318e-05,\n",
       " 1.99438e-05,\n",
       " 1.78221e-05,\n",
       " 1.59261e-05,\n",
       " 1.42319e-05,\n",
       " 1.27178e-05,\n",
       " 1.13649e-05,\n",
       " 1.01558e-05,\n",
       " 9.07544e-06,\n",
       " 8.10997e-06,\n",
       " 7.24721e-06,\n",
       " 6.47623e-06,\n",
       " 5.78727e-06,\n",
       " 5.1716e-06,\n",
       " 4.62143e-06,\n",
       " 4.12979e-06,\n",
       " 3.69045e-06,\n",
       " 3.29785e-06,\n",
       " 2.94701e-06,\n",
       " 2.6335e-06,\n",
       " 2.35334e-06,\n",
       " 2.10299e-06,\n",
       " 1.87926e-06,\n",
       " 1.67934e-06,\n",
       " 1.50069e-06,\n",
       " 1.34104e-06,\n",
       " 1.19838e-06,\n",
       " 1.07089e-06,\n",
       " 9.56966e-07,\n",
       " 8.55161e-07,\n",
       " 7.64186e-07,\n",
       " 6.8289e-07,\n",
       " 6.10242e-07,\n",
       " 5.45323e-07,\n",
       " 4.8731e-07,\n",
       " 4.35468e-07,\n",
       " 3.89142e-07,\n",
       " 3.47744e-07,\n",
       " 3.1075e-07,\n",
       " 2.77691e-07,\n",
       " 2.4815e-07,\n",
       " 2.21751e-07,\n",
       " 1.9816e-07,\n",
       " 1.77079e-07,\n",
       " 1.58241e-07,\n",
       " 1.41407e-07,\n",
       " 1.26364e-07,\n",
       " 1.12921e-07,\n",
       " 1.00908e-07,\n",
       " 9.0173e-08,\n",
       " 8.05801e-08,\n",
       " 7.20078e-08,\n",
       " 6.43474e-08,\n",
       " 5.75019e-08,\n",
       " 5.13847e-08,\n",
       " 4.59182e-08,\n",
       " 4.10333e-08,\n",
       " 3.66681e-08,\n",
       " 3.27672e-08,\n",
       " 2.92813e-08,\n",
       " 2.61663e-08,\n",
       " 2.33826e-08,\n",
       " 2.08951e-08,\n",
       " 1.86722e-08,\n",
       " 1.66858e-08,\n",
       " 1.49107e-08,\n",
       " 1.33245e-08,\n",
       " 1.1907e-08,\n",
       " 1.06403e-08,\n",
       " 9.50835e-09,\n",
       " 8.49682e-09,\n",
       " 7.5929e-09,\n",
       " 6.78515e-09,\n",
       " 6.06332e-09,\n",
       " 5.41829e-09,\n",
       " 4.84188e-09,\n",
       " 4.32678e-09,\n",
       " 3.86649e-09,\n",
       " 3.45516e-09,\n",
       " 3.08759e-09,\n",
       " 2.75912e-09,\n",
       " 2.4656e-09,\n",
       " 2.2033e-09,\n",
       " 1.96891e-09,\n",
       " 1.75945e-09,\n",
       " 1.57227e-09,\n",
       " 1.40501e-09,\n",
       " 1.25554e-09,\n",
       " 1.12197e-09,\n",
       " 1.00261e-09,\n",
       " 8.95953e-10,\n",
       " 8.00639e-10,\n",
       " 7.15464e-10,\n",
       " 6.39351e-10,\n",
       " 5.71335e-10,\n",
       " 5.10555e-10,\n",
       " 4.5624e-10,\n",
       " 4.07704e-10,\n",
       " 3.64331e-10,\n",
       " 3.25573e-10,\n",
       " 2.90937e-10,\n",
       " 2.59987e-10,\n",
       " 2.32328e-10,\n",
       " 2.07613e-10,\n",
       " 1.85526e-10,\n",
       " 1.65789e-10,\n",
       " 1.48152e-10,\n",
       " 1.32391e-10,\n",
       " 1.18307e-10,\n",
       " 1.05721e-10,\n",
       " 9.44743e-11,\n",
       " 8.44239e-11,\n",
       " 7.54426e-11,\n",
       " 6.74168e-11,\n",
       " 6.02448e-11,\n",
       " 5.38358e-11,\n",
       " 4.81086e-11,\n",
       " 4.29906e-11,\n",
       " 3.84172e-11,\n",
       " 3.43302e-11,\n",
       " 3.06781e-11,\n",
       " 2.74145e-11,\n",
       " 2.4498e-11,\n",
       " 2.18918e-11,\n",
       " 1.95629e-11,\n",
       " 1.74818e-11,\n",
       " 1.5622e-11,\n",
       " 1.39601e-11,\n",
       " 1.2475e-11,\n",
       " 1.11478e-11,\n",
       " 9.96191e-12,\n",
       " 8.90213e-12,\n",
       " 7.95509e-12,\n",
       " 7.10881e-12,\n",
       " 6.35255e-12,\n",
       " 5.67675e-12,\n",
       " 5.07284e-12,\n",
       " 4.53317e-12,\n",
       " 4.05092e-12,\n",
       " 3.61997e-12,\n",
       " 3.23487e-12,\n",
       " 2.89073e-12,\n",
       " 2.58321e-12,\n",
       " 2.3084e-12,\n",
       " 2.06283e-12,\n",
       " 1.84338e-12,\n",
       " 1.64727e-12,\n",
       " 1.47203e-12,\n",
       " 1.31543e-12,\n",
       " 1.17549e-12,\n",
       " 1.05044e-12,\n",
       " 9.38691e-13,\n",
       " 8.3883e-13,\n",
       " 7.49593e-13,\n",
       " 6.69849e-13,\n",
       " 5.98588e-13,\n",
       " 5.34909e-13,\n",
       " 4.78004e-13,\n",
       " 4.27152e-13,\n",
       " 3.8171e-13,\n",
       " 3.41103e-13,\n",
       " 3.04815e-13,\n",
       " 2.72388e-13,\n",
       " 2.43411e-13,\n",
       " 2.17516e-13,\n",
       " 1.94376e-13,\n",
       " 1.73698e-13,\n",
       " 1.55219e-13,\n",
       " 1.38707e-13]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lucas/this_script.py\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "# Add the 'functions' folder to the Python path\n",
    "functions_dir = os.path.join(current_dir,'..', 'functions')\n",
    "sys.path.append(functions_dir)\n",
    "\n",
    "# Now you can import the function\n",
    "from Load_data import custom_csv_parser\n",
    "\n",
    "# Use the function\n",
    "data = custom_csv_parser('../Patient.csv', True)\n",
    "#print(data.keys())\n",
    "print(data.keys())\n",
    "pde_keys = ['D1', 'D2', 'I_sc','I_p', 'I_eff', 'G', 'G_sc']\n",
    "patient_keys = ['tau1', 'tau2', 'Ci', 'p2', 'GEZI', 'EGP0', 'Vg', 'taum', 'tausc']\n",
    "patient_keys_si = ['tau1', 'tau2', 'Ci', 'p2', 'Si', 'GEZI', 'EGP0', 'Vg', 'taum', 'tausc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Si0 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    '''Network'''\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__()\n",
    "\n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1] ) for i in range(len(layers)-1)])\n",
    "\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        'Initialization'\n",
    "        for i in range(len(layers) - 1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight, gain=1.0)\n",
    "            nn.init.zeros_(self.linears[i].bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdes(u, nn_dt, data, Si):\n",
    "    D1 = u[0]\n",
    "    D2 = u[1]\n",
    "    I_sc = u[2]\n",
    "    I_p = u[3]\n",
    "    I_eff = u[4]\n",
    "    G = u[5]\n",
    "    G_sc = u[6]\n",
    "    pdes = np.zeros(len(u))\n",
    "    if t == 0:\n",
    "        d = data['Meal_size']\n",
    "        u = data['Bolus']\n",
    "    else:\n",
    "        d = 0\n",
    "        u = data['Steady_insulin']\n",
    "    pdes[0] = nn_dt[0] + D1 / data['taum'] - d\n",
    "    pdes[1] = nn_dt[1] - (D1 - D2) / data['taum'] \n",
    "    pdes[2] = nn_dt[2] + I_sc / data['tau1'] - u / (data['tau1'] * data['Ci'])\n",
    "    pdes[3] = nn_dt[3] - (I_sc - I_p) / data['tau2']\n",
    "    pdes[4] = nn_dt[4] + data['p2'] * I_eff - data['p2'] * Si * I_p\n",
    "    pdes[5] = nn_dt[5] + (data['GEZI'] + I_eff) * G - data['EGP0'] - 1000 * D2 / (data['Vg'] * data['taum'])\n",
    "    pdes[6] = nn_dt[6] - (G - G_sc) / data['tausc']\n",
    "\n",
    "    return pdes\n",
    "\n",
    "class PINN():\n",
    "    def __init__(self, layers):\n",
    "        self.iter = 0\n",
    "\n",
    "        self.Si = torch.tensor([Si0], requires_grad=True, device=device)\n",
    "        self.Si = nn.Parameter(self.Si)\n",
    "        self.nn = DNN(layers).to(device)\n",
    "        self.nn.register_parameter('Si', self.Si)\n",
    "\n",
    "        def data_loss(self, t, data):\n",
    "            u = self.nn(t)  # Outputs 7 x 1 array\n",
    "            loss = 0\n",
    "            for i,key in enumerate(data.keys()):\n",
    "                loss += torch.mean((u[i] - data[key])**2)\n",
    "            return loss\n",
    "\n",
    "        def pde_loss(self, t):\n",
    "            \n",
    "            t = t.clone()\n",
    "            t = t.requires_grad_(True)\n",
    "            u = self.nn(t)\n",
    "            \n",
    "            nn_dt = np.zeros(len(u))\n",
    "\n",
    "            for i in range(len(u)):\n",
    "                nn_dt[i] = autograd.grad(u[i], t, grad_outputs=torch.ones_like(u[i]), create_graph=True)[0]\n",
    "            pdes = pdes(u, nn_dt, data, self.Si)\n",
    "            loss = nn.MSELoss(pdes, torch.zeros_like(pdes))\n",
    "            return loss\n",
    "        \n",
    "        def loss(self, t, data):\n",
    "            return data_loss(self,t, data) + pde_loss(self, t)\n",
    "        \n",
    "        def closure(self):\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.loss(t, data)\n",
    "            loss.backward()\n",
    "            self.iter += 1\n",
    "\n",
    "            if self.iter % 100 == 0:\n",
    "                print(\"Current loss\", loss)\n",
    "                print(self.Si.item())\n",
    "\n",
    "            \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
